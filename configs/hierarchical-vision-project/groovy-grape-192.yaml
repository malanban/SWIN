
DATA:
  DATASET: inat21
  IMG_SIZE: 192
  NUM_WORKERS: 32
MODEL:
  TYPE: swinv2
  NAME: swinv2_base_window12
  DROP_PATH_RATE: 0.2
  SWINV2:
    EMBED_DIM: 128
    DEPTHS: [ 2, 2, 18, 2 ]
    NUM_HEADS: [ 4, 8, 16, 32 ]
    WINDOW_SIZE: 12
TRAIN:
  # Want a global batch size of 2048 because SwinV2 was trained on 16 V100s with batch size 128 (I think)
  # But we are going to use a global batch size of 1024 because it's faster (throughput).
  GLOBAL_BATCH_SIZE: 1024

  # We are using limited epochs based on pre-training configs for imagenet22k
  # Then we will pre-train on 256x256 for 30 epochs
  EPOCHS: 90
  WARMUP_EPOCHS: 5
  WEIGHT_DECAY: 0.1

  # Use 1/4 of original learning rates
  BASE_LR: 1.25e-4
  WARMUP_LR: 1.25e-7
  MIN_LR: 1.25e-6

  HIERARCHICAL_COEFFS: [ 8, 5.65, 4, 2.82, 2, 1.41, 1 ]

EXPERIMENT:
  NAME: groovy-grape-192
  WANDB_ID: 3jcq2v9b

HIERARCHICAL: true
